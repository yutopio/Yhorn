\chapter{Interpolation}
\label{chap:interpolation}

Among various program verification techniques, the interpolation has
been used to extract program behavior as a logical formula.
Especially when one adopts software model checking with
\emph{predicate abstraction}, a technique to abstract a program by a
Boolean valuation of given abstraction predicates, an interpolating
theorem prover plays a role to refine the program abstraction by
computing such predicates along a spurious counterexample that a model
checker discovered.

This process is done by computing a logical separation between the
strongest postcondition and the weakest precondition to the failure
point at every program location.  We apply Craig interpolation for
obtaining such logical separation.

\paragraph{Craig Interpolation}
Given two logical formulas $A$ and $B$ that are inconsistent each
other, we call a new preposition $I$ a \emph{Craig interpolant}
\cite{journals/jsyml/Craig57} between $A$ and $B$ such that $I$ is
implied by $A$ and inconsistent with $B$.  $I$'s vocabulary must be
only free variables that appears in both $A$ and $B$.  If $A \wedge B$
is unsatisfiable, an interpolant $I$ always exists.
\vspace{10pt}

However, by existing interpolating theorem provers, we may not be able
to obtain a simple separation.  Consider the example below.
\begin{align*}
A : (x = 0 \wedge y = 0) \vee (x = 1 \wedge y = 1) \\
B : (x \neq 0 \wedge y = 0) \vee (x \neq 1 \wedge y = 1)
\end{align*}
Although one may be able to obtain $x = y$ as the simplest
interpolation between $A$ and $B$, CSIsat\cite{conf/cav/BeyerZM08}
returns
\begin{align*}
I: \quad & ((((y \leq 0 \wedge -y \leq 0) \vee 1 = x) \wedge 1 \neq x) \vee ((1 \neq x \vee -y \leq -1) \wedge 1 = x) \vee 0 \neq x) \wedge \\
& (((0 \neq x \vee y \leq 1) \wedge (0 = x \vee y \leq 1)) \vee 1 \neq x) \wedge \\
& (((1 \neq x \vee -y \leq -1) \wedge 1 = x) \vee 0 = x) \wedge \\
& ((1 \neq x \wedge 1 = x) \vee (y \leq 0 \wedge -y \leq 0) \vee 0 \neq x) \wedge \\
& (1 = x \vee y \leq 0)
\end{align*}
as a solution.  By the complex solution, a program verification
process may continue to abstract a program by complex predicate,
causing the verification not to terminate.

We put a hypothesis that program invariants are often expressed in
simple expressions. Under the hypothesis, we propose an interpolation
algorithm which returns as simple a solution as possible in terms of
the number of conjunctions and disjunctions.  Our algorithm focuses on
interpolating problems on linear arithmetic.


\section{Background}

For ease of understanding, we review a method of software model
checking with predicate abstraction\cite{conf/cav/GrafS97} in
CounterExample-Guided Abstraction Refinement (CEGAR) framework.

Consider an example program below.

\begin{alltt}
    a = 0;
    b = 0;
\(lbl\):
    // equivalent to assert (a == b);
    if (a != b) fail;
\end{alltt}

At first of the program verification, a predicate abstraction process
translate the input program with empty set of abstraction predicates.
Under such abstraction, the program is abstracted equivalent as a
following listing and a model checker discovers a spurious
counterexample which fails at the the last $fail$ statement.

\begin{alltt}
    \(\star\)
    \(\star\)
\(lbl\):
    if (\(\star\)) fail;
\end{alltt}

In this example, it is sufficient to compute a predicate $I$ that is
satisfied at the label $lbl$, and does not lead to the assertion
failure.  Namely,

\begin{align*}
a = 0 \wedge b = 0 & \vdash I \\
I \wedge (a \neq b) & \vdash \bot
\end{align*}

Here an interpolating theorem prover is used to compute a
Craig interpolant.  A prover may return $I: (a = b)$ as a solution.
By using this newly obtained predicate, the abstraction is refined as follows.

\begin{alltt}
    // p reprensents the state \(a=b\)
    p = true;
\(lbl\):
    if (\(\neg\)p) fail;
\end{alltt}

After this refinement, the model checker does not discover the
counterexample and program verificaction succeeds.

\section{Example}

We briefly explain the mechanism of our algorithm. Consider the
following interpolation problem between $A$ and $B$. Note that
$a,b,x,y$ are variables.

\begin{align*}
\frac
{A: (x \leq a) \wedge (a + 1 \leq y)}
{B: (y \leq b) \wedge (b + 1 \leq x)}
\end{align*}

Here, the linear arithmetic formulas $A$ and $B$ are inconsistent.  The
interpolant $I$ for this problem is $x-y+1 \leq 0$, which is implied by
$A$ and inconsistent with $B$.  The vocabulary of $I$ is $\left\lbrace
x,y \right\rbrace$ and is over $A$'s vocabulary $\left\lbrace x,y,a
\right\rbrace$ and $B$'s $\left\lbrace x,y,b \right\rbrace$.

In computing the interpolant, we make a linear constraint of a
interpolant by applying Farkas's lemma.  First, we assign weight
parameters to every expression in conjunction form.

\begin{align*}
\begin{array}{r r r r r r l}
\lambda_1 : & -a & & +x & & & \leq 0 \\
\lambda_2 : & a & & & -y & +1 & \leq 0 \\
\hline
\lambda_3 : & & -b & & +y & & \leq 0 \\
\lambda_4 : & & b & -x & & +1 & \leq 0
\end{array}
\end{align*}

Because all expressions induce inconsistent as a whole, the sum of
weighted expressions should satisfy the following constraint.

\begin{align*}
\begin{array}{r l l}
- \lambda_1 + \lambda_2 & = 0 \quad & (a) \\
- \lambda_3 + \lambda_4 & = 0 & (b) \\
  \lambda_1 - \lambda_4 & = 0 & (x) \\
- \lambda_2 + \lambda_3 & = 0 & (y) \\
  \lambda_2 + \lambda_4 & > 0 & (\text{constant}) \\
\lambda_1, \ldots, \lambda_4 & \geq 0 & (\text{non-negative weight})
\end{array}
\end{align*}

With the linear constraint among $\lambda_i$ above, the interpolant is
represented as follows by the sum of weighted expressions from the
first formula groups.
\[ (- \lambda_1 + \lambda_2) a + \lambda_1 x - \lambda_2 y + \lambda_2 \leq 0 \]
One of the model of the linear constraint is $\lambda_1 = \lambda_2 =
\lambda_3 = \lambda_4 = 1$, and we obtain $x-y+1 \leq 0$ as a
solution.

% TODO: Better to mention multiple interpolants possible for one
% interpolating problem?


\section{Preliminaries}

Our algorithm aims to compute a Craig interpolant over linear
arithmetic formulas.  We handle a linear expression $e$ which is in a
form:
\[ a_1 x_1 + \cdots + a_n x_n + b \leq 0 \]
This expression contains coefficient parameters $a_1, \ldots, a_n$ and
a constant parameter $b$.  We treat special expressions $\top$ as $0
\leq 0$ and $\bot$ as $1 \leq 0$.  The algorithm input and output is a
formula $\psi$ that consists of conjunctions and/or disjunctions of
linear expressions.
\[ \psi = e | e \wedge e | e \vee e | \top | \bot \]

In the simple interpolation, the algorithm receives two formulas in
$\psi$ and returns an interpolant in $\psi$.

\paragraph{Farkas's Lemma on linear inequalities}
Let a linear inequality $e_i$ be represented as
$a_{i,1} x_1 + \cdots + a_{i,m} x_m <= a_{i,0}$.  Assuming that
$e_1,\cdots,e_n$ implies $e_0$, there exists
$\lambda_1,\cdots,\lambda_n$ that satisfy
\[a_{0,j} = \sum_{i=1}^n \lambda_i a_{i,j} \quad (j=0, \ldots,m)\]
\vspace{10pt}

Especially, our algorithm often uses the special case of Farkas's Lemma.

\paragraph{Farkas's Lemma to induce contradiction}
Let a linear inequality $e_i$ be represented as
$a_{i,1} x_1 + \cdots + a_{i,m} x_m <= a_{i,0}$.  Assuming that
$e_1,\cdots,e_n$ implies $\bot$, there exists
$\lambda_1,\cdots,\lambda_n$ that satisfy
\begin{align*}
& 0 = \sum_{i=1}^n \lambda_i a_{i,j} \quad (j=1, \ldots, m) \wedge \\
& 0 > \sum_{i=1}^n \lambda_i a_{i,0}\
\end{align*}

\section{Algorithm}

Our algorithm aims to construct a relatively small interpolant in a
reasonable computation time for any input.  It also aims to obtain a
common solution among multiple interpolating problems.
To accomplish these goals, the algorithm preserves a set of solution
interpolants instead of one interpolant during the computation of sub
problems, and executes operations over interpolant sets.


\paragraph{Interpolation between conjunctions}
We first present an interpolating algorithm between two conjunctive
sets of linear inequalities $A = \left\lbrace e_1,\ldots,e_m
\right\rbrace$ and $B = \left\lbrace e_{m+1},\ldots,e_{m+n}
\right\rbrace$, where $e_i$ takes a form $\mathbf{a}_i \mathbf{x} + b
\leq 0$.  We assume that conjunctions of $A$ and $B$ are inconsistent.
For later convenience, we refer this interpolating problem as $\left(
A, B \right)$.  Because $A \wedge B$ is contradiction, according to
Craig's Interpolation Theorem, there exists an interpolating linear
expression $e_\star$ which satisfies
\begin{align*}
\left\lbrace e_1,\ldots,e_m \right\rbrace & \vdash e_\star \\
e_\star \wedge \left\lbrace e_{m+1},\ldots,e_{m+n} \right\rbrace & \vdash \bot
\end{align*}

According to the Farkas's lemma, there exists a set of assignments to
$\lambda_i$ to build $\bot$ from a linear combination of expressions
in $A$ and $B$, namely,
\begin{align*}
& \exists \lambda_1, \ldots, \lambda_m. \\
& \sum_{i=1}^{m+n} \lambda_i \mathbf{a}_i = 0\wedge \sum_{i=1}^{m+n} \lambda_i b_i > 0
\end{align*}

Then, a linear combination of $A$'s formula with a weight $\lambda_1,
\ldots, \lambda_m$ becomes an interpolant $I$ between $A$ and $B$.
Note that any assignments to $\lambda_i$ that satisfy the expressions
above will let an expression $I$ to be an interpolant.  Considering
that, it is possible to obtain different interpolants for an
interpolating problem by computing different assignments to
$\lambda_i$.

This enables the algorithm to preserve a set of interpolants by a
linear expression template $a_1 x_1 + \cdots + a_n x_n + b \leq 0$
with a constraint to describe a possible space of coefficient
parameters $a_i$ and the constant parameter $b$.

In practical, we may use linear programming solvers to obtain a model
for $\lambda_i$.  By assigning concrete values to them, we can obtain
a concrete interpolant.


\paragraph{Computing a common interpolant}
Next we consider a method to compute a common interpolant across two
different interpolating problems.  That is, when given two
interpolating problems $\left(A_1, B_1 \right)$ and $\left(A_2, B_2
\right)$, the problem is about to compute the common interpolant $I$,
which is an interpolant for the first problem and the second one at
the same time.

To accomplish this, we simply need to compute the intersection of
interpolant sets.  We use $\mathcal{I}_1$ and $\mathcal{I}_2$ to
denote the interpolant space for the problems $\left(A_1, B_1 \right)$
and $\left(A_2, B_2 \right)$ respectively.  In the previous method, a
set of interpolants was represented by a parameterized linear
expression.
\begin{align*}
\mathcal{I}_1 = \left\lbrace \mathbf{a}_1 \mathbf{x} \leq b_1 \mid
C_1 (\mathbf{a}_1, b_1 ) \right\rbrace \\
\mathcal{I}_2 = \left\lbrace \mathbf{a}_2 \mathbf{x} \leq b_2 \mid
C_2 (\mathbf{a}_2, b_2 ) \right\rbrace
\end{align*}
To compute the intersection between them, it is sufficient to choose
the same assignments of parameters and let them satisfy the both
constraints $C_1$ and $C_2$.
\[ \mathcal{I} = \left\lbrace \mathbf{a} \mathbf{x} \leq b \mid
C_1(\mathbf{a}, b) \wedge C_2(\mathbf{a}, b) \right\rbrace \]

However, if such an assignment of $\mathbf{a}$ and $b$ does not exist
and $\mathcal{I}$ becomes $\emptyset$, no common solution exists
across two problems.


\paragraph{Interpolation between logical formulas}
We now extend the algorithm so that it can handle more general
problems.  We now treat the problem of general linear arithmetic
formulas including disjunctions.

The algorithm takes two formulas over linear expressions as a problem
input.  For convenience, we assume that they are represented in
disjunctive normal form (DNF).  The solution interpolant for this
problem is a logical formula of linear expressions.

First, consider the case of an interpolation problem
$(A_1 \vee A_2, B)$.  When $(A_1 \vee A_2) \wedge B$ is inconsistent,
$A_1 \wedge B$ is necessarily inconsistent.  We use $\mathcal{I}_1$ to
denote the set of interpolants for $(A_1, B)$.  In the same manner, we
use $\mathcal{I}_2$ for the set of interpolants for $(A_2, B)$.  We
may naturally obtain an interpolant set by the intersection
$\mathcal{I} = \mathcal{I}_1 \cap \mathcal{I}_2$ as a solution for
$(A_1 \vee A_2, B)$.  When the algorithm computes $\mathcal{I}$ by the
previous method and it turnes to be $\emptyset$, we need to build a
less simple solution by
\[ \mathcal{I}' = \left\lbrace \left( I_1 \vee I_2 \right) \mid
I_1 \in \mathcal{I}_1 \wedge I_2 \in \mathcal{I}_2 \right\rbrace \]
Any formulas in the set $\mathcal{I}'$ are the interpolant for
$(A_1 \vee A_2, B)$ because
\begin{align*}
\forall I \in \mathcal{I}. & A_1 \vdash I \wedge A_2 \vdash I \wedge I \wedge B \vdash \bot \wedge \\
& FV(I) = FV(A_1) \cap FV(A_2) \cap FV(B)
\end{align*}

Second, consider the case of an interpolation problem
$(A, B_1 \wedge B_2)$.  Similar to the previous case, we use
$\mathcal{I}_1, \mathcal{I}_2$ to denote the set of interpolants for
$(A, B_1)$ and $(A, B_2)$.  We may obtain an common interpolant set by
$\mathcal{I} = \mathcal{I}_1 \cap \mathcal{I}_2$ as a solution for
$(A, B_1 \wedge B_2)$.  If $\mathcal{I} = \emptyset$, we build a less
simple solution by
\[ \mathcal{I}' = \left\lbrace \left( I_1 \wedge I_2 \right) \mid
I_1 \in \mathcal{I}_1 \wedge I_2 \in \mathcal{I}_2 \right\rbrace \]
Any formulas in the set $\mathcal{I}'$ are the interpolant for
$(A, B_1 \wedge B_2)$ because
\begin{align*}
\forall I \in \mathcal{I}. & A \vdash I \wedge I \wedge B_1 \vdash \bot \wedge I \wedge B_2 \vdash \bot \wedge \\
& FV(I) = FV(A) \cap FV(B_1) \cap FV(B_2)
\end{align*}

By combining these two ways of interpolant construction wisely, we may
be able to build a small solution for more general cases like
$(A_1, \ldots, A_n)$ and $(B_1, \ldots, B_n)$.

Recently \cite{conf/cav/AlbarghouthiM13} have independently suggested
a non-deterministic method to combine these ways to build a small
solution in a reasonable time.  It iteratively tests the non-emptiness
of a common interpolant set $\mathcal{I}$ by heuristics.
