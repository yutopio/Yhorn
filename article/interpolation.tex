\chapter{Interpolation}
\label{chap:interpolation}

Among various program verification techniques, when software model
checking with predicate abstraction is adopted, the interpolation is
used to compute the abstraction predicate along a spurious
counterexample that a model checker discovered.  It is done by
computing a Craig interpolant between the strongest postcondition at
the location and the weakest precondition to the failure point at
every program location.

However, by the existing interpolating theorem prover, we may not be
able to obtain a simple solution.  Consider the example below.
\begin{align*}
\varphi_1 : (x = 0 \wedge y = 0) \vee (x = 1 \wedge y = 1) \\
\varphi_2 : (x \neq 0 \wedge y = 0) \vee (x \neq 1 \wedge y = 1)
\end{align*}
Although one may be able to obtain $x = y$ as the simplest
interpolation between $\varphi_1$ and $\varphi_2$,
CSIsat\cite{conf/cav/BeyerZM08} returns
\[ TODO \]
as a solution.

We propose an interpolation algorithm which returns as simple a
solution as possible in terms of the number of conjunctions and
disjunctions.  Our algorithm focuses on interpolating problems on
linear arithmetic and it works in a constructive manner.


\section{Example}

Consider the following interpolation problem.

\begin{align*}
\frac
{A: (x \leq a) \wedge (a + 1 \leq y)}
{B: (y \leq b) \wedge (b + 1 \leq x)}
\end{align*}

Here, the linear arithmetic formulas A and B are inconsistent.  The
interpolant I for this problem is $x-y+1 \leq 0$, which is implied by
$A$ and inconsistent with $B$.  The vocabulary of $I$ is $\left\lbrace
x,y \right\rbrace$ and is over $A$'s vocabulary $\left\lbrace x,y,a
\right\rbrace$ and $B$'s $\left\lbrace x,y,b \right\rbrace$.

In computing the interpolant, we make a linear constraint of a
interpolant by applying Farkas's lemma.  First, we assign weight
parameters to every expression in conjunction form.

\begin{align*}
\begin{array}{r r r r r r l}
\lambda_1 : & -a & & +x & & & \leq 0 \\
\lambda_2 : & a & & & -y & +1 & \leq 0 \\
\hline
\lambda_3 : & & -b & & +y & & \leq 0 \\
\lambda_4 : & & b & -x & & +1 & \leq 0
\end{array}
\end{align*}

Because all expressions induce inconsistent as a whole, the sum of
weighted expressions should satisfy the following constraint.

\begin{align*}
\begin{array}{r l l}
- \lambda_1 + \lambda_2 & = 0 \quad & (a) \\
- \lambda_3 + \lambda_4 & = 0 & (b) \\
  \lambda_1 - \lambda_4 & = 0 & (x) \\
- \lambda_2 + \lambda_3 & = 0 & (y) \\
  \lambda_2 + \lambda_4 & > 0 & (\text{constant}) \\
\lambda_1, \ldots, \lambda_4 & \geq 0 & (\text{non-negative weight})
\end{array}
\end{align*}

With the linear constraint among $\lambda_i$ above, the interpolant is
represented as follows by the sum of weighted expressions from the
first formula groups.
\[ (- \lambda_1 + \lambda_2) a + \lambda_1 x - \lambda_2 y + \lambda_2 \leq 0 \]
One of the model of the linear constraint is $\lambda_1 = \lambda_2 =
\lambda_3 = \lambda_4 = 1$, and we obtain $x-y+1 \leq 0$ as a
solution.

% TODO: Better to mention multiple interpolants possible for one
% interpolating problem?


\section{Preliminaries}

Our algorithm aims to compute a Craig interpolant over linear
arithmetic formulas.


\paragraph{Craig Interpolation}
Given two logical formulas $A$ and $B$ that are inconsistent each
other, we call a new preposition $I$ a \emph{Craig interpolant}
\cite{journals/jsyml/Craig57} between $A$ and $B$ such that $I$ is
implied by $A$ and inconsistent with $B$.  $I$'s vocabulary must be
only free variables that appears in both $A$ and $B$.  If $A \wedge B$
is unsatisfiable, an interpolant $I$ always exists.

Our algorithm handles a linear expression $e$ which is in a form:
\[ a_1 x_1 + \cdots + a_n x_n + b \leq 0 \]
This expression contains coefficient parameters $a_1, \ldots, a_n$ and
a constant parameter $b$.  We treat a special expressions $\top$ as $0
\leq 0$ and $\bot$ as $1 \leq 0$.  The algorithm input and output is a
formula $\psi$ that consists of conjunctions and/or disjunctions of
linear expressions.
\[ \psi = e | e \wedge e | e \vee e | \top | \bot \]

In the simple interpolation, the algorithm receives two formulas in
$\psi$ and returns an interpolant in $\psi$.  For the later extension
to handle a symmetric interpolation problem, the algorithm receives
multiple formulas in $\psi$ and returns intermediate interpolants in
$\psi$.  For computing those interpolants over linear arithmetics, we
use Farkas's Lemma.


\paragraph{Farkas's Lemma on linear inequalities}
Let a linear inequality $e_i$ be represented as $a_i1 x_1 + \cdots +
a_im x_m <= a_i0$.  Assuming that $e_1,\cdots,e_n$ implies $e_0$,
there exists $\lambda_1,\cdots,\lambda_n$ that satisfy $a_0j
=\sum_(i=1)^n \lambda_i * a_ij (j=0, \ldots, m)$.

For the special case of Farkas's Lemma, we can state that:

Assuming that $e_1,\cdots,e_n$ implies $\bot$, there exists
$\lambda_1,\cdots,\lambda_n$ that satisfy $???? TODO $.

Our interpolating algorithm make use of the latter version of Farkas's
Lemma.


\section{Algorithm}

Our algorithm aims to construct a relatively small interpolant in a
reasonable computation time for any input.  It also aims to obtain a
common solution among multiple interpolating problems.  These goals
are set because the algorithm is designed to be used as a sub-routine
for solving Horn clause satisfying problems
that are made while discovering predicates for abstraction
during the program verification.  These advantages of our algorithm
make it possible to return a relatively small predicates for those
problems. To accomplish these aims, the algorithm preserves a set of
interpolants during the computation, and executes operations
over interpolant sets.


\paragraph{Interpolation between conjunctions}
We first present an interpolating algorithm between two conjunctive
sets of linear inequalities $A = \left\lbrace e_1,\ldots,e_m
\right\rbrace$ and $B = \left\lbrace e_{m+1},\ldots,e_{m+n}
\right\rbrace$.  We assume that conjunctions of $A$ and $B$ are
inconsistent.  For later convenience, we call this interpolating
problem $\left( A, B \right)$.  Then there exists an interpolating
linear expression $e_\star$ which satisfies
\begin{align*}
\left\lbrace e_1,\ldots,e_m \right\rbrace & \vdash e_\star \\
e_\star & \nvdash \left\lbrace e_{m+1},\ldots,e_{m+n} \right\rbrace
\end{align*}
For convenience, we assume that any expression $e_i$ have variables
$x_1, \ldots, x_a$.

According to the Farkas's lemma, there exists a set of assignmentqs to
$\lambda_i$, to conclude $\bot$ from a set of linear expressions $A
\cup B$, namely,
\begin{align*}
\exists \lambda_1, \ldots, \lambda_m. \\
a % TODO: coefficient, constant constraints
\end{align*}

Then, a linear combination of $A$'s formula with a weight $\lambda_1,
\ldots, \lambda_m$ becomes an interpolant $I$ between $A$ and $B$.
Note that any assignments to $\lambda_i$ that satisfy the expressions
above will let an expression $I$ to be an interpolant.  Considering
this, it is possible to obtain different interpolants for an
interpolating problem by computing different assignments to
$\lambda_i$.

This enables the algorithm to preserve a set of interpolants by a
linear expression template $a_1 x_1 + \cdots + a_n x_n + b \leq 0$
with a constraint to describe a possible space of coefficient
parameters $a_i$ and the constant parameter $b$.

In practical, we may use linear programming solvers to obtain a model
for $\lambda_i$.  By assigning concrete values to them, we can obtain
a concrete interpolant.


\paragraph{Computing a common interpolant}
Next we consider a method to compute a common interpolant across two
different interpolating problems.  That is, when given two
interpolating problems $\left(A_1, B_1 \right)$ and $\left(A_2, B_2
\right)$, the problem is about to compute the common interpolant I,
which is an interpolant for the first problem and the second one at
the same time.

This problem is used when solving a following Horn clause solving
problem, for instance.
\[
\implies
% TODO
\]

To accomplish this, we simply need to compute the intersection of
interpolant sets.  Let us call the interpolant space for the problems
$\left(A_1, B_1 \right)$ and $\left(A_2, B_2 \right)$,
$\mathcal{I}_1$, and $\mathcal{I}_2$.  In the previous method, a set
of interpolants was represented by a parameterized linear expression.
\begin{align*}
\mathcal{I}_1 = \left\lbrace \mathbf{a}_1 \mathbf{x} \leq b_1 \mid
C_1 (\mathbf{a}_1, b_1 ) \right\rbrace \\
\mathcal{I}_2 = \left\lbrace \mathbf{a}_2 \mathbf{x} \leq b_2 \mid
C_2 (\mathbf{a}_2, b_2 ) \right\rbrace
\end{align*}
To compute the intersection between them, it is sufficient to choose
the same assignments of parameters and let them satisfy the both
constraints $C_1$ and $C_2$.
\[ \mathcal{I} = \left\lbrace \mathbf{a} \mathbf{x} \leq b \mid
C_1(\mathbf{a}, b) \wedge C_2(\mathbf{a}, b) \right\rbrace \]

However, if such an assignment of $\mathbf{a}$ and $b$ does not exist,
no common solution exists across two problems because $\mathcal{I} =
\emptyset$.

By applying this approach, it is also possible to compute a common
interpolant among more than three problems.


\paragraph{Interpolation between logical formulas}
We now extend the algorithm so that it can handle general logical
formulas over linear arithmetic including disjunctions of linear
expressions.  Here the algorithm takes two logical formulas over
linear expressions as a problem input.  For convenience, we assume
that they are represented in disjunctive normal form (DNF).  The
solution for this problem is the interpolant that is a logical formula
of linear expressions.

First, consider the case of an interpolation problem between $A_1 \vee
A_2$ and $B$.  When $(A_1 \vee A_2) \wedge B$ is inconsistent, $A_1
\wedge B$ is necessarily inconsistent.  Let $\mathcal{I}_1$ denote the
set of interpolants for $(A_1, B)$.  In the same manner, let
$\mathcal{I}_2$ denote the set of interpolants for $(A_2, B)$.  We may
naturally obtain an interpolant set by the intersection $\mathcal{I} =
\mathcal{I}_1 \cup \mathcal{I}_2$ as a solution for $(A_1 \vee A_2,
B)$.  When the algorithm computes $\mathcal{I}$ by the previous
method, and it is $\emptyset$, we need to build a less simple solution
by
\[ \mathcal{I}' = \left\lbrace \left( I_1 \vee I_2 \right) \mid
I_1 \in \mathcal{I}_1 \wedge I_2 \in \mathcal{I}_2 \right\rbrace \]
Any formulas in the set $\mathcal{I}'$ are the interpolant for $(A_1
\vee A_2, B)$ because
\begin{align*}
\forall I \in \mathcal{I}. & A_1 \vdash I \wedge A_2 \vdash I \wedge I \nvdash B \wedge \\
& FV(I) = FV(A_1) \cup FV(A_2) \cup FV(B)
\end{align*}

Second, consider the case of an interpolation problem between $A$ and
$B_1 \wedge B_2$.
% TODO:

By combining these two ways of interpolant construction, we may be
able to build a small solution for more general cases like $(A_1,
\ldots, A_n)$ and $(B_1, \ldots, B_n)$.  In order to obtain a small
solution, we need to wisely choose the order of combining an
interpolant set.

\paragraph{Symmetric Interpolation}
TODO
