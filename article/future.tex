\chapter{Future Work}
\label{chap:future}

Our proposed algorithms have still numbers of rooms for further
improvement.  Based on the experiment result from the previous
chapter, here we give raise some analysis and discussion for them.


\paragraph{Space explosion for constraint duplication}
Our experimental implementation did not simplify the linear constraint
for every predicate variable and it caused a linear constraint
unsolvable due to the shortage of space resources by exponential
growth of constraints.  As we described in the algorithm explanation,
it may be possible to perform quantifier elimination for reduce the
size of linear constraints.  On the other hand without the
optimization, the constraint duplication may occur every time a
constraint is generated for the predicate vertices with multiple
successors.

We think that the algorithm with such optimization would not cause a
memory shortage because constraints gently grow according to roughly
the number of vertices.  Although the worst case can be produced by
causing a conjunction split frequently, we consider that it is not the
common case in program verification because of our hypothesis.

In practice, the size of constraint may be limited by simplification,
and additionally, normally the algorithm does not need to preserve the
constraint until the time to build a concrete solution in the end.


\paragraph{Quantifier elimination}
It may be useful to prepare a customized algorithm for performing
quantifier elimination for linear expressions because linear
constraints that are generated from our algorithm have certain pattern
in their format.

In the preliminary experiment before incorporating into MoCHi, although
we have tried the quantifier elimination over integer theory, the
result was unsatisfactory in the sense that simplified formulas had
disjunctions.  It caused the loss of efficiency in solving constraints
and generating unsatisfiable cores.

We have also tried the quantifier elimination over real space, and the
result was better than the integer version.  However, because our
algorithm first treats all linear inequalities over integer space,
more study on the gap between these two are required when we consider
the completeness.


\paragraph{Solving linear constraints}
Although we solve the constraints by a general SMT solver Z3, GNU
Linear Programming Kit (GLPK) may be sufficient for solving
constraints.  The linear programming solver should support sparse
matrix because our algorithm generates constraints that are sparse.


\paragraph{Generation and choice of unsatisfiable core}
We can efficiently build linear constraints for the given $G$ after
the transformation in $\transGraph$~procedure by appropriate inference
from the previous constraint and the SMT query result.  Our Horn
clause solver makes use of Z3 to obtain an unsatisfiable core from the
linear constraints.  However the extraction of an unsatisfiable core
from the proof sums up to a large portion of the whole computation
time in our algorithm.

Although our implementation in this work did not consider such the way
of optimization to reduce the cost of linear constraints queries, it
is necessary to reduce the number of them for reduce the computation
cost.  For instance, the optimization is possible by appropriately
updating $V_\star$ in $\solveHorn$~procedure since the vertices that
are already examined for split does not need to be considered again.

The choice of subset of constraints for an unsatisfiable core is also
important.  In some cases, there exists different multiple cores and
the algorithm returns a different shape of the solution in relation to
the choice of them.  Thus, we need further study to optimize the order
of choice of multiple unsatisfiable cores if ones exist.


\paragraph{Separation of edges}
To transform the graph by $\transGraph$~procedure, there may be
multiple transformations possible.  For example, consider a predicate
variable $P$ and edges $\left\lbrace e_1, e_2, e_3, e_4 \right\rbrace$
that go out from the vertex.  If an unsatisfiable core contains a set
of edges $\left\lbrace e_1, e_2 \right\rbrace$,
$\transGraph$~procedure determines to split those edges by duplicating
$P$.  However at this point, $e_3$ and $e_4$ are free to be connected
to any duplication of $P$.  Although our current algorithm split
$\left\lbrace e_1, e_2, e_3, e_4 \right\rbrace$ into four duplications
$\left\lbrace e_1 \right\rbrace$ to $\left\lbrace e_4 \right\rbrace$,
it is possible to split into $\left\lbrace e_1, e_3\right\rbrace$ and
$\left\lbrace e_2, e_4 \right\rbrace$ for example.  By intelligently
choose this, we may be able to obtain much simpler solution than the
current algorithm.

We suggest to reduce this edge split problem into a graph coloring
problem.  By generating a graph which a set of vertices have an edge
of Horn graph as a label and solve a coloring problem on it, the
algorithm can wisely determine the minimum number of duplications.


\paragraph{Support for Horn clauses with Recursions}
Considering usages of Horn clause solving in program verification, it
is useful to update our algorithm so that it can solve Horn clauses
with recursions as well.  For instance, this makes possible to generate
invariants upon the loop programs on the control flow graphs, rather
than to generate a set of disjunctive constraints
\cite{conf/pldi/BeyerHMR07}.

One known method to let the algorithm support Horn clauses with
recursions is to unroll a looping predicate in the problem
\cite{conf/ppdp/UnnoK09}.  By computing a fixedpoint of the predicate
solution over unrolled programs, it is possible to virtually solve
Horn clauses with recursions as well.


\paragraph{Applications for other work}
In our research, we put a special focus on solving Horn clauses for
dependent type inference in higher-order program verification.
However, solving Horn clauses are becoming common to describe various
kinds of programs' behavior.  For instance in verifying multi-threaded
programs, Horn clauses can describe the conditions of data exchange
between threads, and confirm the validity of semaphores for mutual
exclusion.  We expect our algorithm achieve better performance in
other work when customized into different contexts.

By putting such effort for further improvement on our algorithm, we can
conclude that the algorithm may be able to achieve better performance
in Horn clause solving, and become a necessary tool for program
verification.
