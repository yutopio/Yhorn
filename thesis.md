Introduction
===

Automated theorem proving plays an important role in the modern program verification. Particularly in the context of verification with model checking techniques, an automated theorem prover is used to abstract an infinite set of program states into a finite one so that the model checker can virtually explore all possible program execution paths.

One of the most common methods of such abstraction is predicate abstraction. An automated theorem prover finds appropriate predicates for specific program locations. An execution state at each location is then expressed by a Boolean valuation of the given predicates. If the abstraction is too coarse and a model checker discovers an infeasible error execution path in an abstract system, the prover computes an additional predicate at a specific program location to refine the abstraction. The additional predicate must be satisfied at the location under the execution in the concrete model along the discovered path, and the execution does not lead to the program failure by the same path. After the refinement, the infeasible path is ruled out and no longer found by the model checker.

In order to obtain such a refinement predicate at the specific program location along the infeasible counterexample, one class of automated theorem provers, called an interpolating theorem prover, computes a Craig interpolant between two sets of constraints; the one consists of the constraints from the program entry point to the location, and the other consists of the constraints from the location to the program failure. Those constraints include variable assignments and assertions to be satisfied originated from conditional branches.

However, existing interpolating theorem provers may return too complex a solution which is heavily affected by the constraints of the specific path. This causes the model checker to discover infinitely similar paths which pass the same program location by loops or recursion calls, and not to terminate.

To avoid such situation, it is desired to use an invariant formula for the predicate abstraction. Based on the hypothesis that programâ€™s invariants tend to be simple formulas, we propose a new interpolating algorithm which tries to minimize the number of conjunctions and disjunctions in a solution. The predicate obtained by this algorithm may become similar to the invariant, and the number of trials in model checking may be reduced.

Additionally, we extend our interpolating algorithm to solve a symmetric interpolation problem, in which the algorithm computes predicates at multiple program locations along a path. This enables the abstraction predicates at multiple locations to be updated at the same computation.

Finally, we extend our algorithm to solve Horn clause solving problems. It allows us to obtain predicates which are highly likely to be program invariants, by computing the same abstraction predicates at loops and recursion calls on different passes, and by unifying predicates at the same location over multiple paths. The problems are generated by encoding constraints over execution paths with unknown predicates which describe functions' constraints between parameters and return values.

The rest of the paper is structured as follows. Chapter 2 proposes a new interpolating algorithm to obtain simple solutions, and extends it to solve symmetric interpolation problems. Chapter 3 describes a new Horn clause solving algorithm by extending the previous interpolating algorithm. Chapter 4 shows an experiment and its result to confirm the effect. Chapter 5 mentions related work and future work. As a conclusion, chapter 6 reviews the impact of our research.


Interpolation
===

Among various program verification techniques, when software model checking with predicate abstraction is adopted, the interpolation is used to compute the abstraction predicate along a spurious counterexample that a model checker discovered. It is done by computing a Craig's interpolant between the strongest postcondition at the location and the weakest precondition to the failure point at every program location.

We propose an interpolation algorithm which returns as simple a
solution as possible in terms of the number of conjunctions and
disjunctions.
Our algorithm focuses on interpolating problems on linear arithmetic
and it works in a constructive manner.

Example
---

Consider the following interpolation problem.

A: (x \leq a) \wedge (a + 1 \leq y)
- - - - - -
B: (y \leq b) \wedge (b + 1 \leq x)

Here, the linear arithmetic formulas A and B are inconsistent. The interpolant I for this problem is (x-y+1 \leq 0), which is implied by A and inconsistent with B. The vocabulary of I is {x,y} and is over A's vocabulary {x,y,a} and B's {x,y,b}.

In computing the interpolant, we make a linear constraint of a interpolant by applying Farkas's lemma. First, we assing weight parameters to every expression in conjunction form.

\lambda_1 : -a+x     \leq 0
\lambda_2 :  a  -y+1 \leq 0
- - - - - -
\lambda_3 : -b  +y   \leq 0
\lambda_4 :  b-x  +1 \leq 0

Because all expressions induce inconsistent as a whole, the sum of weighted expressions should satisfy the following constraint.

- \lambda_1 + \lambda_2 = 0 (a)
- \lambda_3 + \lambda_4 = 0 (b)
  \lambda_1 - \lambda_4 = 0 (x)
- \lambda_2 + \lambda_3 = 0 (y)
  \lambda_2 + \lambda_4 > 0 (const)

With the linear constraint among \lambda_i above, the interpolant is represented as follows by the sum of weighted expressions from the first formula groups.

(- \lambda_1 + \lambda_2) a + \lambda_1 x - \lambda_2 y + \lambda_2 \leq 0

One of the model of the linear constraint is \lambda_i = 1 (1 \leq i \leq 4), and we obtain x-y+1 \leq 0 as a solution.

% TODO: Better to mention multiple interpolants possible for one interpolating problem?


Preliminaries
---

Our algorithm aims to compute a Craig's interpolant over linear arithmetic formulas.

\paragraph{Craig's Interpolation}
Given two logical formulas A and B that are inconsistent each other, we call a new preposition I a Craig's interpolant between A and B such that $I$ is implied by $A$ and inconsistent with $B$. $I$'s vocabulary must be only free variables that appears in both $A$ and $B$.

Our algorithm handles a linear expression $e$ which is in a form:
\[ a_1 x_1 + .. + a_n x_n + b \leq 0 \]
This expression contains coefficient parameters $a_1, \ldots, a_n$ and a constant parameter $b$. We treat a special expressions $\top$ as $0 \leq 0$ and $\bot$ as $1 \leq 0$. The algorithm input and output is a formula $\psi$ that consists of conjunctions and/or disjunctions of linear expressions.
\[ \psi = e | e \wedge e | e \vee e | \top | \bot \]

In the simple interpolation, the algorithm receives two formulas in $\psi$ and returns an interpolant in $\psi$. For the later extension to handle a symmetric interpolation problem, the algorithm receives multiple formulas in $\psi$ and returns intermediate interpolants in $\psi$. For computing those interpolants over linear arithmetics, we use Farkas's Lemma.

\paragraph{Farkas's Lemma on linear inequalities}

Let a linear inequality $e_i$ be represented as $a_i1 x_1 + \cdots + a_im x_m <= a_i0$. Assuming that $e_1,\cdots,e_n$ implies $e_0$, there exists $\lambda_1,\cdots,\lambda_n$ that satisfy $a_0j =\sum_(i=1)^n \lambda_i * a_ij (j=0...m)$.

For the special case of Farkas's Lemma, we can state that:

Assuming that $e_1,\cdots,e_n$ implies $\bot$, there exists $\lambda_1,\cdots,\lambda_n$ that satisfy $???? TODO $.

Our interpolating algorithm make use of the latter version of Farkas's Lemma.


Algorithm
---

Our algorithm aims to construct a relatively small interpolant in a reasonable computation time for any input.
It also aims to obtain a common solution among multiple interpolating problems.
When the algorithm is used as a sub-routine for solving Horn clause satisfying problems that are made
while discovering predicates for abstraction during the program verification.,
these advantages of our algorithm make it possible to return a relatively small predicates for
those problems. To accomplish these aims, the algorithm preserves a set of interpolants during the computation, and define and execute operations over interpolant sets.

\paragraph{Interpolation between conjunctions} We first present an interpolating algorithm between two conjunctive sets of linear inequalities $A = \left\lbrace e_1,\ldots,e_m \right\rbrace$ and $B = \left\lbrace e_{m+1},\ldots,e_{m+n} \right\rbrace$. We assume that conjunctions of $A$ and $B$ are inconsistent. Then there exists an interpolating linear expression $\e_star$ which satisfies
\begin{align*}
\left\lbrace e_1,\ldots,e_m \right\rbrace & \vdash e_\star \\
e_\star & \nvdash \left\lbrace e_{m+1},\ldots,e_{m+n} \right\rbrace \\
\end{align*}
For convenience, we assume that any expression $e_i$ have variables $x_1, \ldots, x_a$.

According to the Farkas's lemma,
there exists a set of assignments to $\lambda_i$, to conclude \bot from a set of linear expressions $A \cup B$, namely,
\begin{align*}
\lambda_1, ... m exists. \\
% TODO: coefficient, constant constraints
\end{align*}

Then, a linear combination of $A$'s formula with a weight $\lambda_1, \ldots, \lambda_m$ becomes an interpolant I between $A$ and $B$.  Note that any assignments to $\lambda_i$ that satisfy the expressions above will let an expression I to be an interpolant.  Considering this, it is possible to obtain different interpolants for a interpolating problem by computing different assignments to $\lambda_i$.

This enables the algorithm to preserve a set of interpolants by a linear expression template $a_1 x_1 + \cdots + a_n x_n + b \leq 0$ with a constraint to describe a possible space of coefficient parameters $a_i$ and the constant parameter $b$.

In practical, we may use linear programming solvers to obtain a model for $\lambda_i$. By assigning concrete values to them, we can obtain a concrete interpolant.

\paragraph{Computing a common interpolant}